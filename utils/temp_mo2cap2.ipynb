{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python extract_mo2cap2.py --original_dir F:\\mo2cap2_dataset --destination_dir F:\\extracted_mo2cap2_dataset --dataset_type TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import json\n",
    "import scipy.io\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "import io \n",
    "from PIL import Image\n",
    "\n",
    "def write_json(path, data):\n",
    "    \"\"\"Save data into a json file\n",
    "    Arguments:\n",
    "        path {str} -- path where to save the file\n",
    "        data {serializable} -- data to be stored\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(path, str)\n",
    "    with open(path, 'w') as out_file:\n",
    "        json.dump(data, out_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 지정된 경로를 찾을 수 없습니다: 'F:\\\\mo2cap2_data_temp\\\\TrainSet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    169\u001b[0m \t\t\t\u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(weipeng_json_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1:04}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(fpath[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m8\u001b[39m], frame_idx)))\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m \n\u001b[0;32m    172\u001b[0m \t\u001b[38;5;66;03m# Converting the TrainSet\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m chunk_path \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    175\u001b[0m \n\u001b[0;32m    176\u001b[0m \t\t\u001b[38;5;66;03m# Making the directories\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \t\tchunk_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(destination_dir, chunk_path[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m])\n\u001b[0;32m    179\u001b[0m \t\t\u001b[38;5;28;01mif\u001b[39;00m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tar.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 지정된 경로를 찾을 수 없습니다: 'F:\\\\mo2cap2_data_temp\\\\TrainSet'"
     ]
    }
   ],
   "source": [
    "\n",
    "# parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)\n",
    "# parser.add_argument('--original_dir', help='Original Directory of Dataset', required=True, default=None)\n",
    "# parser.add_argument('--destination_dir', help='Destination Directory', required=True, default=None)\n",
    "# parser.add_argument('--dataset_type', help='One of TRAIN, VAL, TEST', required=True, default=None)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "dict_args = dict(\n",
    "\toriginal_dir = r'F:\\mo2cap2_data_temp',\n",
    "\tdestination_dir = r'F:\\mo2cap2_data_temp_extracted',\n",
    "\tdataset_type = 'TRAIN'\n",
    ")\n",
    "\n",
    "dataset_dir = dict_args[\"original_dir\"]\n",
    "destination_dir = dict_args[\"destination_dir\"]\n",
    "\n",
    "if(dict_args[\"dataset_type\"] == \"TRAIN\"):\n",
    "\tdataset_dir = os.path.join(dataset_dir, \"TrainSet\")\n",
    "\tdestination_dir = os.path.join(destination_dir, \"TrainSet\")\n",
    "elif(dict_args[\"dataset_type\"] == \"TEST\"):\n",
    "\tdataset_dir = os.path.join(dataset_dir, \"TestSet\")\n",
    "\tdestination_dir = os.path.join(destination_dir, \"TestSet\")\n",
    "elif(dict_args[\"dataset_type\"] == \"VAL\"):\n",
    "\tdataset_dir = os.path.join(dataset_dir, \"ValSet\")\n",
    "\tdestination_dir = os.path.join(destination_dir, \"ValSet\")\n",
    "else:\n",
    "\tprint(\"DataSet directory not in options\")\n",
    "\tquit()\n",
    "\n",
    "if(not os.path.exists(destination_dir)):\n",
    "\tos.mkdir(destination_dir)\n",
    "\n",
    "if(dict_args[\"dataset_type\"] == \"TEST\"):\n",
    "\t\n",
    "\t# Converting the TestSet\n",
    "\n",
    "\t# Creating Segments for frames\n",
    "\n",
    "\tactions = ['walking','sitting','crawling','crouching', 'boxing', 'dancing', 'stretching', 'waving', 'total']\n",
    "\n",
    "\tolek = [np.arange(158,818),\n",
    "\t\t\tnp.arange(1017,1213),\n",
    "\t\t\tnp.arange(2432,2841),\n",
    "\t\t\tnp.arange(818,1017), \n",
    "\t\t\tnp.arange(1469,1639),\n",
    "\t\t\tnp.arange(1639,2184),\n",
    "\t\t\tnp.arange(2184,2432),\n",
    "\t\t\tnp.arange(1213,1469),\n",
    "\t\t\tnp.arange(158,2841)]\n",
    "\n",
    "\tweipeng = [np.concatenate([np.arange(387, 654), np.arange(1086, 1461), np.arange(1867, 2040)], axis=0),\n",
    "\t\t\t\tnp.concatenate([np.arange(654,877), np.arange(1535,1867)], axis=0),\n",
    "\t\t\t\tnp.concatenate([np.arange(877,1086), np.arange(3019,3168)], axis=0),\n",
    "\t\t\t\tnp.arange(2883,3019),\n",
    "\t\t\t\tnp.concatenate([np.arange(1461,1535), np.arange(2040,2215)], axis=0),\n",
    "\t\t\t\tnp.arange(2215,2741),\n",
    "\t\t\t\tnp.arange(2741,2883),\n",
    "\t\t\t\tnp.arange(3168,3289),\n",
    "\t\t\t\tnp.arange(387,3289)]\n",
    "\n",
    "\t# Creating the directories\n",
    "\n",
    "\torig_dir_path = dataset_dir\n",
    "\tdestination_path = destination_dir\n",
    "\n",
    "\tolek_dir_path = os.path.join(orig_dir_path, 'olek_outdoor')\n",
    "\tweipeng_dir_path = os.path.join(orig_dir_path, 'weipeng_studio')\n",
    "\tolek_dest_path = os.path.join(destination_path, 'olek_outdoor')\n",
    "\tweipeng_dest_path = os.path.join(destination_path, 'weipeng_studio')\n",
    "\tolek_rgba_path = os.path.join(olek_dest_path, 'rgba')\n",
    "\tolek_json_path = os.path.join(olek_dest_path, 'json')\n",
    "\tweipeng_rgba_path = os.path.join(weipeng_dest_path, 'rgba')\n",
    "\tweipeng_json_path = os.path.join(weipeng_dest_path, 'json')\n",
    "\n",
    "\tif(not os.path.exists(olek_dir_path)):\n",
    "\t\tos.mkdir(olek_dir_path)\n",
    "\tif(not os.path.exists(weipeng_dir_path)):\n",
    "\t\tos.mkdir(weipeng_dir_path)\n",
    "\tif(not os.path.exists(olek_dest_path)):\n",
    "\t\tos.mkdir(olek_dest_path)\n",
    "\tif(not os.path.exists(weipeng_dest_path)):\n",
    "\t\tos.mkdir(weipeng_dest_path)\n",
    "\tif(not os.path.exists(olek_rgba_path)):\n",
    "\t\tos.mkdir(olek_rgba_path)\n",
    "\tif(not os.path.exists(olek_json_path)):\n",
    "\t\tos.mkdir(olek_json_path)\n",
    "\tif(not os.path.exists(weipeng_rgba_path)):\n",
    "\t\tos.mkdir(weipeng_rgba_path)\n",
    "\tif(not os.path.exists(weipeng_json_path)):\n",
    "\t\tos.mkdir(weipeng_json_path)\n",
    "\n",
    "\t# Loading the SciPy files\n",
    "\n",
    "\toleks = scipy.io.loadmat(os.path.join(orig_dir_path, 'olek_outdoor_gt.mat'))\n",
    "\tweipengs = scipy.io.loadmat(os.path.join(orig_dir_path, 'weipeng_studio_gt.mat'))\n",
    "\n",
    "\toleks_p3d = oleks['pose_gt']\n",
    "\tweipengs_p3d = weipengs['pose_gt']\n",
    "\n",
    "\tdict_olek = {}\n",
    "\tdict_weipeng = {}\n",
    "\n",
    "\t# Matching the actions to frames\n",
    "\n",
    "\tfor i, segment in enumerate(olek):\n",
    "\t\tif actions[i] != 'total':\n",
    "\t\t\tfor j in segment:\n",
    "\t\t\t\tdict_olek.update({j : actions[i]})\n",
    "\n",
    "\tfor i, segment in enumerate(weipeng):\n",
    "\t\tif actions[i] != 'total':\n",
    "\t\t\tfor j in segment:\n",
    "\t\t\t\tdict_weipeng.update({j : actions[i]})\n",
    "\n",
    "\t# Creating and Copying the JSON files\n",
    "\n",
    "\tfor fpath in os.listdir(olek_dir_path):\n",
    "\t\tframe_idx = int(fpath[-8:-4])\n",
    "\t\tif frame_idx in dict_olek.keys():\n",
    "\t\t\tshutil.copy(os.path.join(olek_dir_path, fpath), olek_rgba_path)\n",
    "\t\t\tp3d = oleks_p3d[frame_idx-158] # Frame offset when reading from the .mat file\n",
    "\t\t\tdict_json_info = {\n",
    "\t\t\t\t#'Head': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(np.array([0.0, 0.0, 0.0], np.float64))},\n",
    "\t\t\t\t'Neck': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[0])},\n",
    "\t\t\t\t'LeftArm': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[1])},\n",
    "\t\t\t\t'LeftForeArm': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[2])},\n",
    "\t\t\t\t'LeftHand': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[3])},\n",
    "\t\t\t\t'RightArm':  {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[4])},\n",
    "\t\t\t\t'RightForeArm':  {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[5])},\n",
    "\t\t\t\t'RightHand':  {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[6])},\n",
    "\t\t\t\t'LeftUpLeg': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[7])},\n",
    "\t\t\t\t'LeftLeg': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[8])},\n",
    "\t\t\t\t'LeftFoot': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[9])},\n",
    "\t\t\t\t'LeftToeBase': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[10])},\n",
    "\t\t\t\t'RightUpLeg': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[11])},\n",
    "\t\t\t\t'RightLeg': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[12])},\n",
    "\t\t\t\t'RightFoot': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[13])},\n",
    "\t\t\t\t'RightToeBase': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[14])},\n",
    "\t\t\t\t'action': dict_olek[frame_idx]\n",
    "\t\t\t\t}\n",
    "\t\t\twrite_json(os.path.join(olek_json_path, \"{0}{1:04}.json\".format(fpath[:-8], frame_idx)), dict_json_info)\n",
    "\t\t\tprint(os.path.join(olek_json_path, \"{0}{1:04}.json\".format(fpath[:-8], frame_idx)))\n",
    "\n",
    "\tfor fpath in os.listdir(weipeng_dir_path):\n",
    "\t\tframe_idx = int(fpath[-8:-4])\n",
    "\t\tif frame_idx in dict_weipeng.keys():\n",
    "\t\t\tshutil.copy(os.path.join(weipeng_dir_path, fpath), weipeng_rgba_path)\n",
    "\t\t\tp3d = weipengs_p3d[frame_idx-387] # Frame offset when reading from the .mat file\n",
    "\t\t\tdict_json_info = {\n",
    "\t\t\t\t#'Head': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(np.array([0.0, 0.0, 0.0], np.float64))},\n",
    "\t\t\t\t'Neck': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[0])},\n",
    "\t\t\t\t'LeftArm': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[1])},\n",
    "\t\t\t\t'LeftForeArm': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[2])},\n",
    "\t\t\t\t'LeftHand': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[3])},\n",
    "\t\t\t\t'RightArm':  {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[4])},\n",
    "\t\t\t\t'RightForeArm':  {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[5])},\n",
    "\t\t\t\t'RightHand':  {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[6])},\n",
    "\t\t\t\t'LeftUpLeg': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[7])},\n",
    "\t\t\t\t'LeftLeg': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[8])},\n",
    "\t\t\t\t'LeftFoot': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[9])},\n",
    "\t\t\t\t'LeftToeBase': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[10])},\n",
    "\t\t\t\t'RightUpLeg': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[11])},\n",
    "\t\t\t\t'RightLeg': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[12])},\n",
    "\t\t\t\t'RightFoot': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[13])},\n",
    "\t\t\t\t'RightToeBase': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(p3d[14])},\n",
    "\t\t\t\t'action': dict_weipeng[frame_idx]\n",
    "\t\t\t\t}\n",
    "\t\t\twrite_json(os.path.join(weipeng_json_path, \"{0}{1:04}.json\".format(fpath[:-8], frame_idx)), dict_json_info)\n",
    "\t\t\tprint(os.path.join(weipeng_json_path, \"{0}{1:04}.json\".format(fpath[:-8], frame_idx)))\n",
    "else:\n",
    "\n",
    "\t# Converting the TrainSet\n",
    "\n",
    "\tfor chunk_path in os.listdir(dataset_dir):\n",
    "\n",
    "\t\t# Making the directories\n",
    "\n",
    "\t\tchunk_dir = os.path.join(destination_dir, chunk_path[:-5])\n",
    "\t\tif(os.path.exists(f\"{chunk_dir}.tar.gz\")):\n",
    "\t\t\tcontinue\n",
    "\t\tchunk_rgba_dir = os.path.join(destination_dir, chunk_path[:-5], 'rgba')\n",
    "\t\tchunk_json_dir = os.path.join(destination_dir, chunk_path[:-5], 'json')\n",
    "\n",
    "\t\tif(not os.path.exists(chunk_dir)):\n",
    "\t\t\tos.mkdir(chunk_dir)\n",
    "\t\tif(not os.path.exists(chunk_rgba_dir)):\n",
    "\t\t\tos.mkdir(chunk_rgba_dir)\n",
    "\t\tif(not os.path.exists(chunk_json_dir)):\n",
    "\t\t\tos.mkdir(chunk_json_dir)\n",
    "\n",
    "\t\t# Opening the h5py file to make the images and json files (if not full)\n",
    "\n",
    "\t\tif len(os.listdir(chunk_rgba_dir)) != 1000 and len(os.listdir(chunk_json_dir)) != 1000:\n",
    "\t\t\twith h5py.File(os.path.join(dataset_dir, chunk_path), 'r') as chunk:\n",
    "\t\t\t\tprint(os.path.join(dataset_dir, chunk_path))\n",
    "\t\t\t\tfor i in range(len(chunk['Images'])):\n",
    "\t\t\t\t\tjson_d_path = os.path.join(chunk_json_dir, '{0}_{1:06}.json'.format(chunk_path[:-5], i))\n",
    "\t\t\t\t\trgba_d_path = os.path.join(chunk_rgba_dir, '{0}_{1:06}.png'.format(chunk_path[:-5], i))\n",
    "\t\t\t\t\tif(not os.path.exists(json_d_path) and not os.path.exists(rgba_d_path)):\n",
    "\t\t\t\t\t\timg = Image.fromarray(chunk['Images'][i].transpose(1, 2, 0))\n",
    "\t\t\t\t\t\tp2d = chunk[\"Annot2D\"][i]\n",
    "\t\t\t\t\t\tp3d = chunk[\"Annot3D\"][i]\n",
    "\t\t\t\t\t\tdict_json_info = {\n",
    "\t\t\t\t\t\t\t#'Head': {'2d': list(np.array([0.0, 0.0], np.float64)), '3d': list(np.array([0.0, 0.0, 0.0], np.float64))},\n",
    "\t\t\t\t\t\t\t'Neck': {'2d': list(p2d[0]), '3d': list(p3d[0])},\n",
    "\t\t\t\t\t\t\t'LeftArm': {'2d': list(p2d[1]), '3d': list(p3d[1])},\n",
    "\t\t\t\t\t\t\t'LeftForeArm': {'2d': list(p2d[2]), '3d': list(p3d[2])},\n",
    "\t\t\t\t\t\t\t'LeftHand': {'2d': list(p2d[3]), '3d': list(p3d[3])},\n",
    "\t\t\t\t\t\t\t'RightArm':  {'2d': list(p2d[4]), '3d': list(p3d[4])},\n",
    "\t\t\t\t\t\t\t'RightForeArm':  {'2d': list(p2d[5]), '3d': list(p3d[5])},\n",
    "\t\t\t\t\t\t\t'RightHand':  {'2d': list(p2d[6]), '3d': list(p3d[6])},\n",
    "\t\t\t\t\t\t\t'LeftUpLeg': {'2d': list(p2d[7]), '3d': list(p3d[7])},\n",
    "\t\t\t\t\t\t\t'LeftLeg': {'2d': list(p2d[8]), '3d': list(p3d[8])},\n",
    "\t\t\t\t\t\t\t'LeftFoot': {'2d': list(p2d[9]), '3d': list(p3d[9])},\n",
    "\t\t\t\t\t\t\t'LeftToeBase': {'2d': list(p2d[10]), '3d': list(p3d[10])},\n",
    "\t\t\t\t\t\t\t'RightUpLeg': {'2d': list(p2d[11]), '3d': list(p3d[11])},\n",
    "\t\t\t\t\t\t\t'RightLeg': {'2d': list(p2d[12]), '3d': list(p3d[12])},\n",
    "\t\t\t\t\t\t\t'RightFoot': {'2d': list(p2d[13]), '3d': list(p3d[13])},\n",
    "\t\t\t\t\t\t\t'RightToeBase': {'2d': list(p2d[14]), '3d': list(p3d[14])},\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t\tif(not os.path.exists(rgba_d_path)):\n",
    "\t\t\t\t\t\t\timg.save(rgba_d_path)\n",
    "\t\t\t\t\t\t\tprint(rgba_d_path)\n",
    "\t\t\t\t\t\tif(not os.path.exists(json_d_path)):\n",
    "\t\t\t\t\t\t\twrite_json(json_d_path, dict_json_info)\n",
    "\t\t\t\t\t\t\tprint(json_d_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
